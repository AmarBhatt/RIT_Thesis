Preprocess:

	640x480 --> 83x83
	Grayscale
	Remove background (median subtraction)


DAQN (Deep Apprenticeship Q-Network):

	Input: 83x83x? (? = 1-4)

	1st layer (CONV): activation - TANH
		16 feature maps (19x19)
		16 filters (8x8) (stride 4)
		4x4 max-pooling

	2nd layer (CONV): activation - TANH
		32 feature maps (8x8)
		32 filters (4x4)
		2x2 max-pooling

	3rd layer (FC):
		input: 2048
		output: 256

	Output layer (FC):
		input: 256
		output: 3 (actions)
			soft-max non-linearity

	cost = squared sum of the difference of q(s,a) from network (so array of 3 values) and q*(s,a) from expert (1-hot encoded array of real action chosen)
	Learning rate is done with AdaGrad

	Trained with: expert trajectories (s,a)
	Hyperparameters: gamma (discount rate), nu (learning rate)



DARN (Deep Apprenticeship Reward Network):

	Same structure as DQN
	cost = L2 norm of difference of output of DARN (before softmax for action chosen of the input state given) and expected reward
		expected reward = q(s,a)[before softmax] - gamma*max(q(s',a')[before softmax])
		These q(s,a) and q(s',a') come from trained DQN
		Need two instances of trained DQN, one for q(s,a) [current state] and one for q(s',a') [next state]

	Trained with: Random data (probably needs to have some sequence) of agent following random policy (s,a,s')


Use DARN with greedy output to put in a state and get an action


QUESTIONS: How do we train/feed in episodes?
			Iterate over all episodes (sets of data) and call train function